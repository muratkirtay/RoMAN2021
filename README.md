# RoMAN2021
A repository for reproducing the results presented in R0-MAN-2021 submission.

> **Abstract:** Forming trust in a biological or artificial interaction partner that provides reliable strategies and employing the learned strategies to scaffold another agent are critical problems that are often addressed separately in human-robot and robot-robot interaction studies. In this paper, we provide a unified approach to address these issues in robot-robot interaction settings. To be concrete, we present a trust-based affective computational account of scaffolding while performing a sequential visual recalling task. In that, we endow the Pepper humanoid robot with cognitive modules of auto-associative memory and internal reward generation to implement the trust model. The former module is an instance of a cognitive function with an associated neural cost determining the cognitive load of performing visual memory recall. The latter module uses this cost to generate an internal reward signal to facilitate neural cost-based reinforcement learning (RL) in an interactive scenario involving online instructors with different guiding strategies: reliable, less-reliable, and random. These cognitive modules allow the Pepper robot to assess the instructors based on the average cumulative reward it can collect and choose the instructor that helps reduce its cognitive load most as the trustworthy one. After determining the trustworthy instructor, the Pepper robot is recruited to be a caregiver robot to guide a perceptually limited infant robot (i.e., the Nao robot) that performs the same task. In this setting, we equip the Pepper robot with a simple theory of mind module that learns the state-action-reward associations by observing the infant robot’s behavior and guides the learning of the infant robot, similar to when it went through the online agent-robot interactions. The experiment results on this robot-robot interaction scenario indicate that the Pepper robot as a caregiver leverages the decision-making policies – obtained by interacting with the trustworthy instructor– to guide the infant robot to perform the same task efficiently. Overall, this study suggests how robotic-trust can be grounded in human-robot or robot-robot interactions based on cognitive load, and be used as a mechanism to choose the right scaffolding agent for effective knowledge transfer.

## Folder and file descriptions
+ **Assets:** this folder contains various assets to create the scene for visual processing, visual patterns to train associative memory, etc.  
+ **Source:** the source files to run the experiments, preprocess assets, postprocess the data, and visualize the results. 
+ **Data:** this contains subfolders for the results in .pkl format. Check visualization in the 
+ **Source folder** to obtain the result for the various runs. Note that base folder refers to the experimental results with caregiver agent that has no trust component and theory of mind module.
+ **Figures:** the subfolders host the figures generated for the article. Note that the scripts in Source/visualization folder can be employed to generate more figures for different runs to depict q matrices, cumulative rewards, etc. 
+ **TheRobotTrustScaffolding.mp4:** A video file to show the experiment demo with the Pepper and Nao robots.  
+ **authorCopyTrustModel.pdf:** The author copy of ICDL 2021 paper.  
+ **versions_py.txt:** a text file contains the version numbers of the python packages for running the scripts.